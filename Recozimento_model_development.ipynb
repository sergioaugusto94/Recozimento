{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Recozimento_model_development.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergioaugusto94/Recozimento/blob/main/Recozimento_model_development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZOI-WPKasB3"
      },
      "source": [
        "# Algoritmo Recozimento\n",
        "\n",
        "O algoritmo desenvolvido busca prever a qualidade do processo de tratamento térmico do recozimento. É necessário ter mais detalhes do modelo de negócio, mas a principio, o algoritmo foi elaborado de forma a minimizar a quantidade de recozimentos ideais que são classificados como ruins (no caso, a peça é descartada incorretamente, o que gera redução do lucro da companhia) e também minimizar a quantidade de recozimentos ruins que são classificados como bons (nesse caso, a peça é tratada como conforme e pode falar em trabalho, o que pode acarretar prejuizos à imagem da empresa). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx5W-jebasB6"
      },
      "source": [
        "O processo de tratamento de dados foi uma das etapas que mostrou melhores avanços no aprendizado final do algoritmo. A performance dos algoritmos foi testada a cada processo de tratamento de dados. Nesse trabalho, foram utilizados 4 processos de tratamento de dados: \n",
        "obs: Em todas as etapas foi utilizada a imputação por regressão.\n",
        "\n",
        "**1- Imputação com correlação das colunas + Valor de corte para valores faltantes = 600 + Valor de corete para valores nulos = 200**\n",
        "    A lógica por detrás dessa etapa foi que colunas com muitos dados faltantes ou nulos não seriam relevantes no aprendizado do algoritmo. Por isso essas foram descartadas. No entanto, como pode se observar no gráfico abaixo, o resultado de descartar muitas colunas resultou em um aprendizado muito fraco. \n",
        "\n",
        "**2- Imputação com correlação das colunas + Valor de corte para valores faltantes = 700**\n",
        "    Nesse processo de tratamento, foram utilizadas mais colunas que no processo anterior, já que se permitiram usar colunas com mais de 700 dados faltantes como input no aprendizado do algoritmo. Essa abordagem trouxe ganhos consideráveis no aprendizado. \n",
        "    \n",
        "**3- Imputação com correlação das colunas + Valor de corte para valores faltantes = 750 + Relação 'temper rolling' e recozimento 'ideal'**\n",
        "    Já nesse processo, aumentou-se ainda mais as colunas para input no algoritmo, bem como houve um tratamento especial da coluna 'temper_rolling', já que esta última possuia uma correlação muito boa com o recozimento ideal, o que poderia ajudar no aprendizado do algoritmo. Apesar da alta correlação, esse processo trouxe ganhos de aprendizado mínimos. \n",
        "        \n",
        "**4- Processo 3 + Técnica de Subamostragem**\n",
        "    Nesse procedimento, tentou-se minimizar os registros das classes que mais aparecem na base de dados. Essa etapa mostrou-se bastante promissora, na qual alguns algoritmos atingiram scores superiores a 90%.\n",
        "    \n",
        "Cada um dos processos é explicado mais a fundo ao longo do trabalho. A imagem abaixo traz a evolução dos algoritmos em função do processo de tratamento de dados usado. \n",
        "    \n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6GtVGpuasB8"
      },
      "source": [
        "**Tratamento da base de dados**\n",
        "\n",
        "A primeira etapa do tratamento de dados é a importação dos dados a partir das planilhas csv.\n",
        "Foram unidas as planilhas com a data de realização do experimento e os dados de cada experimento. \n",
        "Após essa etapa, foram filtrados os experimentos realizados em agosto de 2020."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llzmnyh2asB9"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "date = pd.read_csv('data_experimentos.csv')\n",
        "dataset = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "dataset= pd.concat([date,dataset], axis=1, join='inner')\n",
        "\n",
        "#Filtrando os dados\n",
        "dataset = dataset.loc[dataset['ano']==2020]\n",
        "dataset = dataset.loc[dataset['mes']==8]\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lyh99-cUasB-"
      },
      "source": [
        "**Remoção de campos que não são relevantes para o algoritmo**\n",
        "\n",
        "1- A coluna 'product-type' possui um único valor, logo não tem capacidade de aprimorar o aprendizado do algoritmo.\n",
        "\n",
        "2- As colunas 'experimento', 'ano' e 'mes' não tem relação com a qualidade do recozimento. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuQuIBf-asB-"
      },
      "source": [
        "#%% Valores unicos na coluna/ Não relevantes para o algoritmo\n",
        "dataset.loc[dataset['product-type'].notnull(),'product-type'].describe()\n",
        "dataset = dataset.drop(columns = ['product-type','experimento','ano','mes']) "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TgcQjajasB_"
      },
      "source": [
        "**Remoção de colunas com muitos valores faltantes/ muito valores 0.**\n",
        "\n",
        "1- Muitos valores faltantes prejudicam/ não influenciam na aprendizagem do algoritmo. Foram feitos vários testes do valor de corte de uma determinada coluna. Caso uma coluna tenha um número de registros vázios acima desse limite, a coluna é deletada da base de dados. Foram feitos vários testes para estipular o melhor valor de corte:\n",
        "    \n",
        "    a) Valor de corte 600\n",
        "    \n",
        "    b) Valor de corte 700\n",
        "    \n",
        "    c) Valor de corte 750\n",
        "\n",
        "2- Muitos valores 0 impoem uma tendência indezejada no algoritmo. Foi estabelecido que uma coluna que tinha menos de 200 valores não nulos seria eliminada. \n",
        "\n",
        "obs: \n",
        "\n",
        "    . Foi realizado o tratamento dos dados faltantes, que na base de dados eram registrados como '?'\n",
        "\n",
        "    . Os dados 'formability' eram tratados como string e estes foram convertidos para float\n",
        "    \n",
        "No código é salvada as colunas trabalhadas para ser utilizada posteriormente no script do Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl7DnWgTasCA"
      },
      "source": [
        "# Valores Faltantes\n",
        "dataset.replace('?', np.nan, inplace=True)\n",
        "dataset['formability'] = dataset['formability'].astype(float)\n",
        "dataset.isnull().sum()\n",
        "\n",
        "colunas_faltantes = []\n",
        "for i in range(len(dataset.columns.values)):\n",
        "    if dataset.isnull().sum()[i] > 750:\n",
        "        colunas_faltantes.append(dataset.columns.values[i])\n",
        "\n",
        "# Muitos Zeros\n",
        "colunas_zeros = []\n",
        "# for coluna in dataset.columns.values:\n",
        "#     if np.count_nonzero(dataset[coluna]) < 200:\n",
        "#         colunas_zeros.append(coluna)\n",
        "\n",
        "pickle.dump(colunas_faltantes,open('colunas_faltantes.txt','wb'))\n",
        "        \n",
        "dataset = dataset.drop(columns = colunas_faltantes+colunas_zeros)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erK8OjRiasCB"
      },
      "source": [
        "Foi verificado que existe uma correlação muito boa entre os valores da coluna 'temper_rolling' e o recozimento classificado como 'ideal', como observado na figura abaixo: \n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "Dessa forma, para evitar que essa correlação seja perdida nas etapas posteriores do tratamento de dados, na qual vamos fazer a imputação de valores por meio da técnica de regressão, vamos preencher os registros vazios dessa coluna com o valor 'NULA'.\n",
        "\n",
        "O mesmo foi feito com as colunas 'bf', 'bl', 'oil', 'cbond' e 'bt' para verificar se há algum benefício na performance do algoritmo em utilizar essas técnicas no tramento de dados. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbL_1xg1asCC"
      },
      "source": [
        "# Foi observado um padrão forte entre a classe 'Ideal' com a coluna 'Temper_rolling'\n",
        "\n",
        "dataset.loc[dataset['temper_rolling'].isnull(),'temper_rolling'] = 'NULA'\n",
        "dataset.loc[dataset['bf'].isnull(),'bf'] = 'NULA'\n",
        "dataset.loc[dataset['bl'].isnull(),'bl'] = 'NULA'\n",
        "dataset.loc[dataset['oil'].isnull(),'oil'] = 'NULA'\n",
        "dataset.loc[dataset['cbond'].isnull(),'cbond'] = 'NULA'\n",
        "dataset.loc[dataset['bt'].isnull(),'bt'] = 'NULA'\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMZEC28QasCD"
      },
      "source": [
        "Correção das colunas 'thick','width' e 'len' para que o algoritmo entenda a variável como um float"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrYd-OIDasCD"
      },
      "source": [
        "cor_strings = ['thick','width','len']\n",
        "for coluna in cor_strings:\n",
        "    for i in range(len(dataset)):\n",
        "        dataset.loc[i,coluna]=float(dataset[coluna][i].replace('_',''))\n",
        "        "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uekS4q91asCE"
      },
      "source": [
        "Pegando a localização de cada uma das colunas que possuem variáveis categóricas para realização do Label Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ9jAeDAasCE"
      },
      "source": [
        "dataset_notnull = pd.DataFrame(dataset.dropna())\n",
        "\n",
        "coluna_string = []\n",
        "dataset_notnull.loc[0,:]=dataset.iloc[0,:].values\n",
        "for i in range(dataset_notnull.shape[1]):\n",
        "    if type(dataset_notnull.iloc[0,:][i])==str:\n",
        "        coluna_string.append(i)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eE1k0g2asCE"
      },
      "source": [
        "Fazendo o Label Encoder, para converter as variáveis categóricas em variáveis discretas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxOQfZhzasCF"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "dataset_notnull=dataset_notnull.iloc[:,:].values\n",
        "\n",
        "for i in coluna_string:\n",
        "    dataset_notnull[:,i]=label_encoder.fit_transform(dataset_notnull[:,i])\n",
        "\n",
        "dataset_notnull = pd.DataFrame(dataset_notnull,columns=dataset.columns.values,dtype = 'float64')\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLZBIiLhasCF"
      },
      "source": [
        "**Imputação de dados pela alta correlação entre as colunas**\n",
        "\n",
        "Existe uma grande correlação entre as colunas 'surface-quality', 'condition', 'family' e 'steel', por esse motivo, optou-se pelo preenchimento dos valores faltantes conforme o código abaixo. Esse procedimento seguiu a lógica mostrada na figura abaixo, na qual percebemos que quando 'surface-quality' é 'E', a maior parte de 'conditions' é 'S'. Dessa forma, toda a coluna conditions é preenchida com 'S'.\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQUablmkasCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5b8a6ec-eebd-4e7a-d370-b2ac0eae6dc1"
      },
      "source": [
        "# Existe uma grande correlação entre Surface Quality, Condition e Steel\n",
        "dataset.loc[dataset['surface-quality']=='E','condition']='S' \n",
        "dataset.loc[dataset['surface-quality']=='D','condition']='S' \n",
        "dataset.loc[dataset['surface-quality']=='F','condition']='S'\n",
        "dataset.loc[dataset['condition']=='A','surface-quality']='G' \n",
        "dataset.loc[dataset['steel']=='A','condition']='S' \n",
        "dataset.loc[dataset['steel']=='M','surface-quality']='G' \n",
        "dataset.loc[(dataset['surface-quality']=='G') & (dataset['condition']=='A'),'steel']='R'\n",
        "dataset.loc[(dataset['steel']=='R') & (dataset['condition']=='S'),'surface-quality']='E'\n",
        "dataset.loc[dataset['family']=='TN','steel']='A'\n",
        "\n",
        "dataset.isnull().sum()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "exp_id               0\n",
              "family             691\n",
              "steel               50\n",
              "carbon               0\n",
              "hardness             0\n",
              "temper_rolling       0\n",
              "condition          121\n",
              "formability        291\n",
              "strength             0\n",
              "non-ageing         713\n",
              "surface-quality    136\n",
              "bf                   0\n",
              "bt                   0\n",
              "bw/me              617\n",
              "bl                   0\n",
              "cbond                0\n",
              "shape                0\n",
              "thick                0\n",
              "width                0\n",
              "len                  0\n",
              "oil                  0\n",
              "bore                 0\n",
              "recozimento          0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aipVvsY3asCH"
      },
      "source": [
        "Mesmo preenchendo os registros com o método acima, ainda assim, algumas colunas possuiam alguns valores vazios. Para resolver esse problema, utilizou-se a técnica de imputação de valores a partir da Regressão Linear Multipla."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qOsuo3_asCI"
      },
      "source": [
        "**Imputação por Regressão Linear Multipla**\n",
        "\n",
        "1- O primeiro passo para implementar essa técnica é localizar as colunas que possuem valores faltantes. \n",
        "\n",
        "2- Logo após, definimos a função que adiciona ao DataFrame de trabalho colunas de imputação, na qual somente a linha com valores faltantes é preenchida com valores aleatórios da coluna em questão.\n",
        "\n",
        "3- Com a função definida, cada coluna que possui valores faltantes é chamada em um loop. As colunas criadas, com o final '_imputation', são copiadas das colunas originais. Logo após a função do item anterior é chamada, que fará o preenchimento dos registros faltantes com valores aleatórios. No mesmo loop os valores categóricos são transformados em variáveis discretas, através do Label Encoder. \n",
        "\n",
        "4- No loop seguinte, fazemos a substituição dos valores contidos nas colunas originais da base de dados, pelos valores contidos nas colunas de final '_imputation'. Ao mesmo tempo, fazemos o procedimento de Label Encoder nas variáveis categóricas da coluna original. \n",
        "\n",
        "5- Em seguida, criamos um novo DataFrame, que irá armazenar os valores calculados pela regressão linear. Nesse modelo de regressão, ele utiliza todas as colunas como entrada, com excessão da coluna 'recozimento' (para não haver nenhuma influência na hora de realizar o treinamento dos algoritmos) para realizar o cálculo dos valores faltantes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpyMgpTzasCJ"
      },
      "source": [
        "colunas_faltantes = []\n",
        "for i in range(len(dataset.isnull().sum())):\n",
        "    if dataset.isnull().sum()[i]>1:\n",
        "        colunas_faltantes.append(dataset.isnull().sum().index[i])\n",
        "\n",
        "def random_input(df,coluna):\n",
        "    numeros_faltantes = df[coluna].isnull().sum()\n",
        "    valores_observados = df.loc[df[coluna].notnull(),coluna]\n",
        "    df.loc[df[coluna].isnull(),coluna+'_imputation']=np.random.choice(valores_observados,numeros_faltantes,replace=True)\n",
        "    #cria uma coluna no DF, com valores aleatorios de idade, onde não há o valor de idade\n",
        "    return df\n",
        "\n",
        "for coluna in colunas_faltantes:\n",
        "    encoded=dataset.iloc[:,dataset.columns.get_loc(coluna)].values\n",
        "    dataset[coluna+'_imputation'] = dataset[coluna]\n",
        "    dataset = random_input(dataset,coluna)\n",
        "    if coluna != 'formability':\n",
        "        encoded=label_encoder.fit_transform(dataset[coluna+'_imputation'].values)\n",
        "        dataset[coluna+'_imputation'] = dataset[coluna+'_imputation'].replace(dataset[coluna+'_imputation'].values.tolist(),encoded)\n",
        "\n",
        "for coluna in dataset.columns.values:\n",
        "    if colunas_faltantes.count(coluna)>0:\n",
        "        encoded = dataset.iloc[:,dataset.columns.get_loc(coluna+'_imputation')].values\n",
        "        dataset[coluna] = dataset[coluna].replace(dataset[coluna].values.tolist(),encoded)\n",
        "    if type(dataset[coluna][0]) == str or coluna == 'family':\n",
        "        encoded=label_encoder.fit_transform(dataset[coluna].values)\n",
        "        dataset[coluna] = dataset[coluna].replace(dataset[coluna].values.tolist(),encoded)\n",
        "    \n",
        "from sklearn.linear_model import LinearRegression\n",
        "deter_data = pd.DataFrame(columns = ['Det' + coluna for coluna in colunas_faltantes])\n",
        "for coluna in colunas_faltantes:\n",
        "    deter_data['Det'+coluna]=dataset[coluna+'_imputation']\n",
        "    parameters = list(set(dataset.columns)-set(colunas_faltantes)-{coluna+'_imputation'}-{'recozimento'})\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X=dataset[parameters],y=dataset[coluna+'_imputation'])\n",
        "    \n",
        "    deter_data.loc[dataset[coluna].isnull(),'Det'+coluna] = model.predict(dataset[parameters])[dataset[coluna].isnull()]\n",
        "    dataset[coluna]=dataset[coluna+'_imputation']\n",
        "    dataset = dataset.drop(columns=coluna+'_imputation')"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bl5lD_qasCK"
      },
      "source": [
        "Fazemos agora o processamento final dos dados, separando os dados de input para o algoritmo:\n",
        "\n",
        "O One Hot Encoder não mostrou ganhos significativos na aprendizagem do algoritmo. Por esse motivo ele foi desconsiderado no script final, pois compromete o processamento dos dados. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gvx2yZtasCK"
      },
      "source": [
        "x_data = dataset.iloc[:,1:dataset.shape[1]-1].values\n",
        "\n",
        "    ##--One Hot Encoder\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# from sklearn.compose import ColumnTransformer\n",
        " \n",
        "# onehotencoder_data=ColumnTransformer(transformers=[('OneHot',OneHotEncoder(),[dataset.columns.get_loc(\"steel\"),dataset.columns.get_loc(\"surface-quality\")])],remainder='passthrough')\n",
        "# x_data = onehotencoder_data.fit_transform(x_data)\n",
        "\n",
        "    ##--Escalonamento dos dados\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "scaler_data = StandardScaler()\n",
        "x_data = scaler_data.fit_transform(x_data)\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch1sDsbyasCL"
      },
      "source": [
        "Aplicando o Label Encoder nas classes: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK7rhrTmasCL"
      },
      "source": [
        "encoded=label_encoder.fit_transform(dataset['recozimento'].values)\n",
        "dataset['recozimento'] = dataset['recozimento'].replace(dataset['recozimento'].values.tolist(),encoded)\n",
        "\n",
        "\n",
        "y_data = dataset.iloc[:,dataset.shape[1]-1].values"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6m0FcLGasCM"
      },
      "source": [
        "**Subamostragem**\n",
        "\n",
        "Como nossa base de dados está um pouco desbalanceada, foi aplicada a técnica de subamostragem Tomek Links de forma a amenizar a componente de tendencia para a classe que mais aparece no banco de dados. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0GtgcxYasCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1ae3833-ced9-4a42-c246-84efa0e41996"
      },
      "source": [
        "from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "tl = TomekLinks(sampling_strategy='all') \n",
        "x_data, y_data = tl.fit_sample(x_data,y_data)\n",
        "\n",
        "np.unique(y_data,return_counts=True)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1, 2]), array([155, 138, 409]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdKXlnJ3asCM"
      },
      "source": [
        "**Tuning dos Algoritmos**\n",
        "\n",
        "Para o caso, foram utilizados os seguintes algoritmos: Random Forest, KNN, SVM, Redes Neurais e Regressão Logistica. \n",
        "O tuning desses algoritmos foi realizado usando o GridSearch.\n",
        "Ao fim do Tuning, foram obtidos os valores dos melhores parâmetros e também o melhor score da combinação dos algoritmos. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmu5fFGTasCN"
      },
      "source": [
        "#%% TUNNIG DOS ALGORITMOS\n",
        "from sklearn.model_selection import GridSearchCV\n",
        " \n",
        "    # -----Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# parametros1 = {'criterion': ['gini','entropy'],\n",
        "#               'n_estimators': [23,14,21],\n",
        "#               'min_samples_split':[3,7,9,5],\n",
        "#               'min_samples_leaf':[1,3,7,2]}\n",
        "# grid_search = GridSearchCV(estimator=RandomForestClassifier(),param_grid=parametros1)\n",
        "# grid_search.fit(x_data,y_data)\n",
        "# melhor_parametro = grid_search.best_params_\n",
        "# melhor_resultado = grid_search.best_score_\n",
        "# print(melhor_parametro)\n",
        "# print(melhor_resultado) \n",
        "\n",
        "    #-----KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# parametros1 = {'n_neighbors': [2,3,5,9,10,11,12,13,14],\n",
        "#               'p': [1,2], 'weights': ['uniform', 'distance'] }\n",
        "# grid_search = GridSearchCV(estimator = KNeighborsClassifier(), param_grid=parametros1)\n",
        "# grid_search.fit(x_data,y_data)\n",
        "# melhor_parametro = grid_search.best_params_\n",
        "# melhor_resultado = grid_search.best_score_\n",
        "# print(melhor_parametro)\n",
        "# print(melhor_resultado)\n",
        "\n",
        "    #-----SVM\n",
        "from sklearn.svm import SVC\n",
        "# parametros1 = {'tol': [0.001,0.0001,0.00001],\n",
        "#               'C': [2.4,2.0,1.8],\n",
        "#               'kernel': ['rbf','linear','poly'],\n",
        "#               }\n",
        "# search_grid = GridSearchCV(estimator=SVC(), param_grid=parametros1)\n",
        "# search_grid.fit(x_data,y_data)\n",
        "# melhor_parametro = search_grid.best_params_\n",
        "# melhor_resultado = search_grid.best_score_\n",
        "# print(melhor_parametro) \n",
        "# print(melhor_resultado) \n",
        "\n",
        "    #----Redes Neurais\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "# parametros1 = {'activation': ['relu','logistic','tanh'],\n",
        "#               'solver': ['adam','sgd'],\n",
        "#               'batch_size': [10,56],\n",
        "#               'hidden_layer_sizes': [(5,5),(5,5,5)]} \n",
        "# search_grid = GridSearchCV(estimator=MLPClassifier(), param_grid=parametros1)\n",
        "# search_grid.fit(x_data,y_data)\n",
        "# melhor_parametro = search_grid.best_params_\n",
        "# melhor_resultado = search_grid.best_score_\n",
        "# print(melhor_parametro) \n",
        "# print(melhor_resultado) "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeeVotX_asCN"
      },
      "source": [
        "**Cross Validation**\n",
        "\n",
        "Nessa etapa, vamos avaliar a performance de cada algoritmo, tendo como base diferentes bases de treinamento. No caso, vamos dividir a base de dados em 8 partes, uma delas será usada como base de teste e, todas as partes serão usadas, alguma hora como base de testes. Esse procedimento será repedido 20 vezes, a cada ciclo, a base de treinamento e a base de teste é embaralhada. Assim, podemos ter uma noção da variância da performance do algoritmo para a base de dados e também se há overfitting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y17H0DMZasCO"
      },
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        " \n",
        "# resultados_random_forest = []\n",
        "# resultados_knn = []\n",
        "# resultados_svm = []\n",
        "# resultados_redes = []\n",
        "#resultados_regressao = []\n",
        "\n",
        "for i in range(20):\n",
        "    kfold = KFold(n_splits = 8, random_state = i, shuffle = True)\n",
        "    \n",
        "# #1-------------------Random Forest\n",
        "#     random_forest = RandomForestClassifier(criterion= 'entropy', min_samples_leaf = 1, min_samples_split = 7, n_estimators = 21)\n",
        "#     scores = cross_val_score(random_forest, x_data,y_data,cv=kfold)\n",
        "#     resultados_random_forest.append(scores.mean())\n",
        "# #2------------------KNN\n",
        "#     knn = KNeighborsClassifier(n_neighbors= 5, p= 1, weights= 'distance')\n",
        "#     scores = cross_val_score(knn, x_data,y_data,cv=kfold)\n",
        "#     resultados_knn.append(scores.mean())\n",
        "# #3------------------SVM\n",
        "#     svm = SVC(C =2.4, kernel= 'rbf', tol = 0.001)\n",
        "#     scores = cross_val_score(svm, x_data,y_data,cv=kfold)\n",
        "#     resultados_svm.append(scores.mean()) \n",
        "# #4------------------Redes\n",
        "#     redes = MLPClassifier(activation = 'relu', batch_size = 10, hidden_layer_sizes = (15, 15), max_iter = 1500, solver = 'adam')\n",
        "#     scores = cross_val_score(redes, x_data,y_data,cv=kfold)\n",
        "#     resultados_redes.append(scores.mean())\n",
        "#5------------------Regressão Logistica\n",
        "#     regressao = LogisticRegression(random_state=1)\n",
        "#     scores = cross_val_score(regressao, x_data,y_data,cv=kfold)\n",
        "#     resultados_regressao.append(scores.mean())"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOfJ_knZasCO"
      },
      "source": [
        "**Análise Estatística do Algoritmo**\n",
        "\n",
        "Vemos abaixo os resultados finais da técnica de Cross Validation. Percebemos que o algoritmo SVM produz os melhores scores e além disso possui o menor desvio padrão, significando que o algoritmo é constante, independente da base de dados que ele receber. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Q-wB2MzuasCP"
      },
      "source": [
        "# np.mean(resultados_random_forest), np.mean(resultados_knn), np.mean(resultados_svm), np.mean(resultados_redes), np.mean(resultados_regressao)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "yVZmdRH0asCP"
      },
      "source": [
        "# np.std(resultados_random_forest), np.std(resultados_knn), np.std(resultados_svm), np.std(resultados_redes), np.std(resultados_regressao)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3xkpF_v2asCQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5244082c-9190-43c9-e5f8-d9f16d90e279"
      },
      "source": [
        "#------DIVISAO DA BASE EM TREINAMENTO E TESTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        " \n",
        "x_data_treinamento, x_data_teste, y_data_treinamento, y_data_teste = train_test_split(x_data, \n",
        "                                                                                      y_data, test_size=0.25, random_state=32) \n",
        " \n",
        " \n",
        " \n",
        "#---------Treinamento/Teste---------\n",
        " \n",
        "#------Random Forest\n",
        "random_forest = RandomForestClassifier(criterion= 'entropy', min_samples_leaf = 1, min_samples_split = 7, n_estimators = 21)\n",
        "random_forest.fit(x_data_treinamento,y_data_treinamento)\n",
        "previsoes_random = random_forest.predict(x_data_teste)\n",
        "print('Random:',accuracy_score(y_data_teste,previsoes_random))\n",
        "prec_random = confusion_matrix(y_data_teste,previsoes_random)\n",
        "print (classification_report(y_data_teste, previsoes_random))\n",
        "print(prec_random)\n",
        "#------KNN\n",
        "knn = KNeighborsClassifier(n_neighbors= 5, p= 1, weights= 'distance')\n",
        "knn.fit(x_data_treinamento,y_data_treinamento)\n",
        "previsoes_knn = knn.predict(x_data_teste)\n",
        "print('\\n KNN',accuracy_score(y_data_teste,previsoes_knn))\n",
        "prec_knn =confusion_matrix(y_data_teste,previsoes_knn)\n",
        "print (classification_report(y_data_teste, previsoes_knn))\n",
        "print(prec_knn)\n",
        "#------SVM\n",
        "svm = SVC(C =2.4, kernel= 'rbf', tol = 0.001)\n",
        "svm.fit(x_data_treinamento,y_data_treinamento)\n",
        "previsoes_svm = svm.predict(x_data_teste)\n",
        "print('\\n SVM',accuracy_score(y_data_teste,previsoes_svm))\n",
        "prec_svm =confusion_matrix(y_data_teste,previsoes_svm)\n",
        "print (classification_report(y_data_teste, previsoes_svm))\n",
        "print(prec_svm)\n",
        "#------Redes\n",
        "rede = MLPClassifier(activation = 'relu', batch_size = 10, hidden_layer_sizes = (15, 15), max_iter = 1500, solver = 'adam')\n",
        "rede.fit(x_data_treinamento,y_data_treinamento)\n",
        "previsoes_rede = rede.predict(x_data_teste)\n",
        "print('\\n Rede',accuracy_score(y_data_teste,previsoes_rede))\n",
        "prec_rede =confusion_matrix(y_data_teste,previsoes_rede)\n",
        "print (classification_report(y_data_teste, previsoes_rede))\n",
        "print(prec_rede)\n",
        "#------Regressão Logistica\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "regressao = LogisticRegression(random_state=1)\n",
        "regressao.fit(x_data_treinamento,y_data_treinamento)\n",
        "previsoes_regressao = regressao.predict(x_data_teste)\n",
        "print('\\n Regressão',accuracy_score(y_data_teste,previsoes_regressao))\n",
        "prec_regressao = confusion_matrix(y_data_teste,previsoes_regressao)\n",
        "print (classification_report(y_data_teste, previsoes_regressao))\n",
        "print(prec_regressao)\n",
        "\n",
        "print('/n',dataset.groupby(['recozimento']).size())"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random: 0.8863636363636364\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.91      0.96        47\n",
            "           1       1.00      0.59      0.74        39\n",
            "           2       0.82      1.00      0.90        90\n",
            "\n",
            "    accuracy                           0.89       176\n",
            "   macro avg       0.94      0.83      0.87       176\n",
            "weighted avg       0.91      0.89      0.88       176\n",
            "\n",
            "[[43  0  4]\n",
            " [ 0 23 16]\n",
            " [ 0  0 90]]\n",
            "\n",
            " KNN 0.8579545454545454\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.87      0.90        47\n",
            "           1       0.85      0.56      0.68        39\n",
            "           2       0.83      0.98      0.90        90\n",
            "\n",
            "    accuracy                           0.86       176\n",
            "   macro avg       0.87      0.80      0.83       176\n",
            "weighted avg       0.86      0.86      0.85       176\n",
            "\n",
            "[[41  2  4]\n",
            " [ 3 22 14]\n",
            " [ 0  2 88]]\n",
            "\n",
            " SVM 0.8977272727272727\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.96      0.98        47\n",
            "           1       0.92      0.59      0.72        39\n",
            "           2       0.85      1.00      0.92        90\n",
            "\n",
            "    accuracy                           0.90       176\n",
            "   macro avg       0.92      0.85      0.87       176\n",
            "weighted avg       0.91      0.90      0.89       176\n",
            "\n",
            "[[45  2  0]\n",
            " [ 0 23 16]\n",
            " [ 0  0 90]]\n",
            "\n",
            " Rede 0.8295454545454546\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.83      0.90        47\n",
            "           1       0.75      0.62      0.68        39\n",
            "           2       0.80      0.92      0.86        90\n",
            "\n",
            "    accuracy                           0.83       176\n",
            "   macro avg       0.84      0.79      0.81       176\n",
            "weighted avg       0.83      0.83      0.83       176\n",
            "\n",
            "[[39  2  6]\n",
            " [ 0 24 15]\n",
            " [ 1  6 83]]\n",
            "\n",
            " Regressão 0.8465909090909091\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.81      0.87        47\n",
            "           1       0.92      0.59      0.72        39\n",
            "           2       0.79      0.98      0.88        90\n",
            "\n",
            "    accuracy                           0.85       176\n",
            "   macro avg       0.89      0.79      0.82       176\n",
            "weighted avg       0.86      0.85      0.84       176\n",
            "\n",
            "[[38  2  7]\n",
            " [ 0 23 16]\n",
            " [ 2  0 88]]\n",
            "/n recozimento\n",
            "0    172\n",
            "1    186\n",
            "2    450\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaSNf828asCR"
      },
      "source": [
        "Temos acima a performance de cada um dos algoritmos usados. O objetivo estipulado para esse trabalho é obter o menor numero de peças que tem recozimento 'ruim' e serem classificadas como peças que tem recozimento 'ideal' e também obter o menor numero de peças que tem recozimento 'ideal' e serem classificadas como peças que tem recozimento 'ruim'. Tendo em vista essa ideia, foram rodados 4 testes para verificar o comportamento de cada algoritmo para classificar cada uma das classes corretamente. Foram geradas assim as tabelas abaixo:  \n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "![image-3.png](attachment:image-3.png)\n",
        "\n",
        "![image-4.png](attachment:image-4.png)\n",
        "\n",
        "![image-5.png](attachment:image-5.png)\n",
        "\n",
        "Para cada tipo de recozimento, foi escolhido o algoritmo mais preciso, que no caso foram:\n",
        "\n",
        "**Ideal** Random Forest\n",
        "\n",
        "**Mediano** Random Forest\n",
        "\n",
        "**Ruim** KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfXUxJQHasCS"
      },
      "source": [
        "Implementação da tomada de decisão em conjunto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "hlD17XRCasCS"
      },
      "source": [
        "prev_final = []\n",
        "\n",
        "for i in range(y_data_teste.shape[0]):\n",
        "    if previsoes_random[i] == 1:\n",
        "        prev_final.append(1)\n",
        "    elif previsoes_knn[i] == 2:\n",
        "        prev_final.append(2)\n",
        "    elif previsoes_random[i] == 0:\n",
        "        prev_final.append(0)\n",
        "    else:\n",
        "        prev_final.append(previsoes_svm[i])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDwFcvRTasCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a031f49-2b3a-4a17-dcbc-67b466f4d4fb"
      },
      "source": [
        "print('\\n Regressão',accuracy_score(y_data_teste,prev_final))\n",
        "prec_final = confusion_matrix(y_data_teste,prev_final)\n",
        "print (classification_report(y_data_teste, prev_final))\n",
        "print(prec_final)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Regressão 0.8863636363636364\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.91      0.96        47\n",
            "           1       1.00      0.59      0.74        39\n",
            "           2       0.82      1.00      0.90        90\n",
            "\n",
            "    accuracy                           0.89       176\n",
            "   macro avg       0.94      0.83      0.87       176\n",
            "weighted avg       0.91      0.89      0.88       176\n",
            "\n",
            "[[43  0  4]\n",
            " [ 0 23 16]\n",
            " [ 0  0 90]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfJ1TruSasCU"
      },
      "source": [
        "**Conclusão**\n",
        "\n",
        "Ao utilizar os algoritmos em conjunto, foi possível melhorar ainda mais a acertividade do processo de classificação, chegando em 91%. \n",
        "A precisão da previsão de recozimentos 'ideais' chegou a 98%, enquanto que a precisão da previsão de recozimentos 'ruins' chegou a 89%. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yzmvrFPasCU"
      },
      "source": [
        "**Treinamento Final e Salvamento do Algoritmo**\n",
        "\n",
        "Vamos agora realizar o treinamento final dos algoritmos usados, agora com a base de dados total e ao fim do treinamento, vamos realizar o salvamento para este poder ser utilizado em uma ferramenta de classificação de recozimento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EMQd74FasCU"
      },
      "source": [
        "import pickle\n",
        "\n",
        "random_forest = RandomForestClassifier(criterion= 'entropy', min_samples_leaf = 1, min_samples_split = 7, n_estimators = 21)\n",
        "random_forest.fit(x_data,y_data)\n",
        "#------KNN\n",
        "knn = KNeighborsClassifier(n_neighbors= 5, p= 1, weights= 'distance')\n",
        "knn.fit(x_data,y_data)\n",
        "#------SVM\n",
        "svm = SVC(C =2.4, kernel= 'rbf', tol = 0.001)\n",
        "svm.fit(x_data,y_data)\n",
        "\n",
        "#------Salvamento do Algoritmo\n",
        "pickle.dump(random_forest,open('random_finalizado.sav','wb'))\n",
        "pickle.dump(knn,open('knn_finalizado.sav','wb'))\n",
        "pickle.dump(svm,open('svm_finalizado.sav','wb'))"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kshuRpCasCV"
      },
      "source": [
        ""
      ],
      "execution_count": 53,
      "outputs": []
    }
  ]
}